{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image info](https://raw.githubusercontent.com/albahnsen/MIAD_ML_and_NLP/main/images/banner_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhoyoso\\.conda\\envs\\gpu_tenv\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhoyoso\\.conda\\envs\\gpu_tenv\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\dhoyoso\\.conda\\envs\\gpu_tenv\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "C:\\Users\\dhoyoso\\.conda\\envs\\gpu_tenv\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.5'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy, pandas\n",
    "numpy.__version__\n",
    "pandas.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENTO CON GLOVE EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dhoyoso\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dhoyoso\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dhoyoso\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\dhoyoso\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import os\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics import r2_score, roc_auc_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Carga de datos de archivo .csv\n",
    "dataTraining = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTraining.zip', encoding='UTF-8', index_col=0)\n",
    "dataTesting = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTesting.zip', encoding='UTF-8', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       ...,\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 1, ..., 0, 0, 0],\n",
       "       [0, 1, 1, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definición de variable de interés (y)\n",
    "dataTraining['genres'] = dataTraining['genres'].map(lambda x: eval(x))\n",
    "le = MultiLabelBinarizer()\n",
    "y_genres = le.fit_transform(dataTraining['genres'])\n",
    "y_genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dhoyoso\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dhoyoso\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Descargar recursos de NLTK\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Crear objeto para lematización\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Obtener stopwords en inglés\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Preprocesar los textos\n",
    "def preprocess_text(text):\n",
    "    # Convertir a minúsculas\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Eliminar stopwords\n",
    "    text = ' '.join(word for word in text.split() if word not in stopwords)\n",
    "    \n",
    "    # Lematización\n",
    "    text = ' '.join(lemmatizer.lemmatize(word) for word in text.split())\n",
    "    \n",
    "    # Eliminar caracteres especiales\n",
    "    text = re.sub(r'[^a-zA-Z ]+', '', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3107    most story single father take eight year  old ...\n",
       "900     serial killer serial killer decides teach secr...\n",
       "6724    womans face sweden  female blackmailer disfigu...\n",
       "4704    executive suite friday afternoon new york  pre...\n",
       "2582    narrow margin los angeles  editor publishing h...\n",
       "                              ...                        \n",
       "8417    family wedding  marriage  wedding    lesson nu...\n",
       "1592    conan destroyer wandering barbarian  conan  al...\n",
       "1723    kismet like tale spun scheherazade  kismet fol...\n",
       "7605    secret nimh mr  brisby  widowed mouse  life ci...\n",
       "215     tinker bell lost treasure tinker bell journey ...\n",
       "Name: sinopsis_processed, Length: 7895, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aplicar preprocesamiento a la matriz de sinopsisarra\n",
    "dataTraining['sinopsis'] = dataTraining.apply(lambda row: row['title'] + '; ' + row['plot'], axis=1)\n",
    "dataTraining['sinopsis_processed'] = dataTraining['sinopsis'].apply(preprocess_text)\n",
    "\n",
    "dataTraining['sinopsis_processed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "vocab_size = 400000\n",
    "oov_token = \"<OOV>\"\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_token)\n",
    "\n",
    "# Realizar la partición de los datos\n",
    "X_train, X_test, y_train_genres, y_test_genres = train_test_split(dataTraining['sinopsis_processed'], y_genres, test_size=0.33, random_state=42)\n",
    "\n",
    "# Tokenizar los datos\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2721,\n",
       "  5614,\n",
       "  3794,\n",
       "  795,\n",
       "  761,\n",
       "  12364,\n",
       "  925,\n",
       "  6367,\n",
       "  3180,\n",
       "  1168,\n",
       "  316,\n",
       "  303,\n",
       "  14,\n",
       "  9999,\n",
       "  3551,\n",
       "  2889,\n",
       "  1762,\n",
       "  925,\n",
       "  62,\n",
       "  1,\n",
       "  324,\n",
       "  13,\n",
       "  21,\n",
       "  1153,\n",
       "  272,\n",
       "  1,\n",
       "  4285,\n",
       "  737,\n",
       "  3619,\n",
       "  536,\n",
       "  1257],\n",
       " [19,\n",
       "  7862,\n",
       "  24,\n",
       "  69,\n",
       "  689,\n",
       "  463,\n",
       "  814,\n",
       "  1,\n",
       "  764,\n",
       "  1,\n",
       "  26577,\n",
       "  837,\n",
       "  639,\n",
       "  2673,\n",
       "  206,\n",
       "  6,\n",
       "  6244,\n",
       "  4141,\n",
       "  1889,\n",
       "  1,\n",
       "  344,\n",
       "  101,\n",
       "  851,\n",
       "  5968,\n",
       "  4552],\n",
       " [859,\n",
       "  398,\n",
       "  13931,\n",
       "  384,\n",
       "  982,\n",
       "  118,\n",
       "  2079,\n",
       "  1846,\n",
       "  1317,\n",
       "  14817,\n",
       "  10,\n",
       "  221,\n",
       "  9066,\n",
       "  13931,\n",
       "  150,\n",
       "  158,\n",
       "  720,\n",
       "  203,\n",
       "  7,\n",
       "  67,\n",
       "  51,\n",
       "  1301,\n",
       "  24,\n",
       "  55,\n",
       "  9,\n",
       "  13931,\n",
       "  14923,\n",
       "  274,\n",
       "  2158,\n",
       "  1196,\n",
       "  1297,\n",
       "  729,\n",
       "  5,\n",
       "  720,\n",
       "  482,\n",
       "  375,\n",
       "  2268,\n",
       "  388,\n",
       "  3709,\n",
       "  3321,\n",
       "  1,\n",
       "  1645,\n",
       "  1962,\n",
       "  13931,\n",
       "  248,\n",
       "  140,\n",
       "  947,\n",
       "  17,\n",
       "  2260,\n",
       "  1768,\n",
       "  1301,\n",
       "  2110,\n",
       "  150,\n",
       "  79,\n",
       "  277,\n",
       "  13931,\n",
       "  1597,\n",
       "  4,\n",
       "  2500,\n",
       "  247,\n",
       "  5,\n",
       "  27,\n",
       "  6513,\n",
       "  887,\n",
       "  1317,\n",
       "  301,\n",
       "  9066,\n",
       "  480,\n",
       "  52,\n",
       "  28514,\n",
       "  304,\n",
       "  743,\n",
       "  15],\n",
       " [101,\n",
       "  6642,\n",
       "  1149,\n",
       "  3622,\n",
       "  1,\n",
       "  5883,\n",
       "  7176,\n",
       "  631,\n",
       "  1,\n",
       "  1,\n",
       "  166,\n",
       "  640,\n",
       "  2733,\n",
       "  232,\n",
       "  7197,\n",
       "  16,\n",
       "  28,\n",
       "  1,\n",
       "  76,\n",
       "  70,\n",
       "  17,\n",
       "  888,\n",
       "  240,\n",
       "  7197,\n",
       "  12,\n",
       "  239],\n",
       " [1,\n",
       "  77,\n",
       "  15652,\n",
       "  168,\n",
       "  10382,\n",
       "  927,\n",
       "  232,\n",
       "  10657,\n",
       "  4789,\n",
       "  380,\n",
       "  239,\n",
       "  6287,\n",
       "  2,\n",
       "  9,\n",
       "  25,\n",
       "  40,\n",
       "  1605,\n",
       "  3059,\n",
       "  674,\n",
       "  2144,\n",
       "  1590,\n",
       "  2197,\n",
       "  3530,\n",
       "  17855,\n",
       "  9040,\n",
       "  1783,\n",
       "  1546,\n",
       "  14767,\n",
       "  1260,\n",
       "  1,\n",
       "  263,\n",
       "  6183,\n",
       "  2943,\n",
       "  3336,\n",
       "  162,\n",
       "  3182,\n",
       "  342,\n",
       "  2315,\n",
       "  2362,\n",
       "  717,\n",
       "  6574,\n",
       "  876,\n",
       "  3470,\n",
       "  734],\n",
       " [4118,\n",
       "  3765,\n",
       "  3070,\n",
       "  1193,\n",
       "  5951,\n",
       "  205,\n",
       "  1,\n",
       "  3009,\n",
       "  19981,\n",
       "  2470,\n",
       "  3070,\n",
       "  164,\n",
       "  244,\n",
       "  306,\n",
       "  1596,\n",
       "  61,\n",
       "  188,\n",
       "  126,\n",
       "  65,\n",
       "  157,\n",
       "  348,\n",
       "  35,\n",
       "  18863,\n",
       "  1970,\n",
       "  53,\n",
       "  145,\n",
       "  61,\n",
       "  107,\n",
       "  35,\n",
       "  149,\n",
       "  6235,\n",
       "  2802,\n",
       "  1266,\n",
       "  74,\n",
       "  1,\n",
       "  7771,\n",
       "  61,\n",
       "  897,\n",
       "  15095,\n",
       "  204,\n",
       "  3070,\n",
       "  111,\n",
       "  177,\n",
       "  575,\n",
       "  1,\n",
       "  4080,\n",
       "  12161,\n",
       "  1,\n",
       "  1281,\n",
       "  842,\n",
       "  162,\n",
       "  3928,\n",
       "  7290,\n",
       "  2522,\n",
       "  61],\n",
       " [6287,\n",
       "  16374,\n",
       "  223,\n",
       "  285,\n",
       "  2057,\n",
       "  4249,\n",
       "  1532,\n",
       "  160,\n",
       "  1913,\n",
       "  22782,\n",
       "  37,\n",
       "  5,\n",
       "  55,\n",
       "  186,\n",
       "  4,\n",
       "  720,\n",
       "  77,\n",
       "  2488,\n",
       "  160,\n",
       "  5911,\n",
       "  23,\n",
       "  78,\n",
       "  2555,\n",
       "  88,\n",
       "  747,\n",
       "  9414,\n",
       "  11513,\n",
       "  77,\n",
       "  222,\n",
       "  86,\n",
       "  777,\n",
       "  4116,\n",
       "  13080,\n",
       "  77,\n",
       "  553,\n",
       "  155,\n",
       "  4409,\n",
       "  5328,\n",
       "  6868,\n",
       "  205,\n",
       "  1411,\n",
       "  2297,\n",
       "  14499,\n",
       "  37,\n",
       "  242,\n",
       "  24441,\n",
       "  12880,\n",
       "  1131,\n",
       "  795,\n",
       "  7891,\n",
       "  6167,\n",
       "  619,\n",
       "  14499,\n",
       "  1502,\n",
       "  417,\n",
       "  802,\n",
       "  202,\n",
       "  619,\n",
       "  128,\n",
       "  5328,\n",
       "  6868,\n",
       "  205,\n",
       "  45,\n",
       "  4409,\n",
       "  13080,\n",
       "  564,\n",
       "  747,\n",
       "  19230,\n",
       "  77,\n",
       "  285,\n",
       "  5133,\n",
       "  86,\n",
       "  33,\n",
       "  1916,\n",
       "  370,\n",
       "  7488,\n",
       "  1682,\n",
       "  37,\n",
       "  5,\n",
       "  13,\n",
       "  720,\n",
       "  49,\n",
       "  223,\n",
       "  2057,\n",
       "  2209,\n",
       "  442,\n",
       "  2641,\n",
       "  104,\n",
       "  77],\n",
       " [3033, 2506, 483, 12872, 405, 428, 719, 1983, 1256],\n",
       " [179,\n",
       "  632,\n",
       "  13908,\n",
       "  19895,\n",
       "  1652,\n",
       "  339,\n",
       "  3254,\n",
       "  695,\n",
       "  343,\n",
       "  16922,\n",
       "  88,\n",
       "  1,\n",
       "  1,\n",
       "  24901,\n",
       "  1054,\n",
       "  10,\n",
       "  1577,\n",
       "  1581,\n",
       "  42,\n",
       "  11774,\n",
       "  680,\n",
       "  789,\n",
       "  69,\n",
       "  2178,\n",
       "  9,\n",
       "  347,\n",
       "  1812,\n",
       "  1174,\n",
       "  459,\n",
       "  687,\n",
       "  1381,\n",
       "  1652,\n",
       "  2586,\n",
       "  10709,\n",
       "  374,\n",
       "  2543,\n",
       "  35,\n",
       "  1816,\n",
       "  4159,\n",
       "  48,\n",
       "  324,\n",
       "  1639,\n",
       "  2628,\n",
       "  4864,\n",
       "  2537,\n",
       "  2344,\n",
       "  1,\n",
       "  1,\n",
       "  632,\n",
       "  1,\n",
       "  1,\n",
       "  108,\n",
       "  28,\n",
       "  674,\n",
       "  2469,\n",
       "  262,\n",
       "  44,\n",
       "  70,\n",
       "  195,\n",
       "  163,\n",
       "  5023,\n",
       "  3254,\n",
       "  3],\n",
       " [5400,\n",
       "  442,\n",
       "  2820,\n",
       "  63,\n",
       "  47,\n",
       "  34,\n",
       "  7842,\n",
       "  21,\n",
       "  10,\n",
       "  108,\n",
       "  150,\n",
       "  85,\n",
       "  927,\n",
       "  3058,\n",
       "  3,\n",
       "  784,\n",
       "  178,\n",
       "  442,\n",
       "  793,\n",
       "  29,\n",
       "  3972,\n",
       "  15,\n",
       "  164,\n",
       "  421,\n",
       "  1556,\n",
       "  111,\n",
       "  3291,\n",
       "  178,\n",
       "  8811,\n",
       "  226,\n",
       "  442,\n",
       "  2820,\n",
       "  266,\n",
       "  24,\n",
       "  614,\n",
       "  194]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_sequences[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 300\n",
    "padding_type='post'\n",
    "truncation_type='post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X_test_padded = pad_sequences(X_test_sequences,maxlen=max_length, \n",
    "                               padding=padding_type, truncating=truncation_type)\n",
    "X_train_padded = pad_sequences(X_train_sequences,maxlen=max_length, padding=padding_type, \n",
    "                       truncating=truncation_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1645, 17990,    22, ...,     0,     0,     0],\n",
       "       [   37,  4900,   692, ...,     0,     0,     0],\n",
       "       [ 6690, 13583,     2, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [ 3413, 13362,  2846, ...,     0,     0,     0],\n",
       "       [ 1473,  2750,  7230, ...,     0,     0,     0],\n",
       "       [  271, 10508,    34, ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "embeddings_index = {}\n",
    "f = open(\"glove.6B.300d.txt\", encoding=\"UTF-8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, max_length))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 300, 300)          8935200   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 300, 300)          541200    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 300)               541200    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               38528     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 24)                3096      \n",
      "=================================================================\n",
      "Total params: 10,059,224\n",
      "Trainable params: 1,124,024\n",
      "Non-trainable params: 8,935,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Flatten, Input, Conv1D, MaxPooling1D, Concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "embedding_layer = Embedding(input_dim=len(word_index) + 1,\n",
    "                            output_dim=max_length,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_length,\n",
    "                            trainable=False)\n",
    "\n",
    "# Obtener el número de géneros\n",
    "num_genres = y_train_genres.shape[1]\n",
    "\n",
    "model = Sequential([\n",
    "    embedding_layer,\n",
    "    Bidirectional(LSTM(150, return_sequences=True)), \n",
    "    Bidirectional(LSTM(150)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(num_genres, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',optimizer='adamax', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n",
      "Epoch 1/600\n",
      "21/21 [==============================] - 7s 312ms/step - loss: 0.1490 - accuracy: 0.4944 - val_loss: 0.2238 - val_accuracy: 0.4133\n",
      "Epoch 2/600\n",
      "21/21 [==============================] - 6s 308ms/step - loss: 0.1477 - accuracy: 0.4876 - val_loss: 0.2254 - val_accuracy: 0.4098\n",
      "Epoch 3/600\n",
      "21/21 [==============================] - 7s 335ms/step - loss: 0.1437 - accuracy: 0.4954 - val_loss: 0.2254 - val_accuracy: 0.3960\n",
      "Epoch 4/600\n",
      "21/21 [==============================] - 7s 338ms/step - loss: 0.1407 - accuracy: 0.4903 - val_loss: 0.2292 - val_accuracy: 0.4217\n",
      "Epoch 5/600\n",
      "21/21 [==============================] - 7s 335ms/step - loss: 0.1393 - accuracy: 0.5026 - val_loss: 0.2294 - val_accuracy: 0.3937\n",
      "Epoch 6/600\n",
      "21/21 [==============================] - 7s 342ms/step - loss: 0.1366 - accuracy: 0.4959 - val_loss: 0.2305 - val_accuracy: 0.4048\n",
      "Epoch 7/600\n",
      "21/21 [==============================] - 7s 351ms/step - loss: 0.1321 - accuracy: 0.5071 - val_loss: 0.2314 - val_accuracy: 0.4006\n",
      "Epoch 8/600\n",
      "21/21 [==============================] - 7s 350ms/step - loss: 0.1308 - accuracy: 0.5063 - val_loss: 0.2350 - val_accuracy: 0.3642\n",
      "Epoch 9/600\n",
      "21/21 [==============================] - 7s 356ms/step - loss: 0.1290 - accuracy: 0.5069 - val_loss: 0.2379 - val_accuracy: 0.3588\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "%load_ext tensorboard\n",
    "\n",
    "early_stopping = EarlyStopping(monitor=\"val_accuracy\", patience = 5)\n",
    "\n",
    "log_folder = 'logs'\n",
    "callbacks = [\n",
    "            early_stopping,\n",
    "            TensorBoard(log_dir=log_folder)\n",
    "            ]\n",
    "num_epochs = 600\n",
    "history = model.fit(X_train_padded, y_train_genres, batch_size = 256, epochs=num_epochs, validation_data=(X_test_padded, y_test_genres),callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82/82 [==============================] - 3s 37ms/step - loss: 0.2379 - accuracy: 0.3588\n",
      "Test accuracy : 0.3587874174118042\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test_padded,y_test_genres)\n",
    "print('Test accuracy :', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTesting['sinopsis'] = dataTesting.apply(lambda row: row['title'] + '; ' + row['plot'], axis=1)\n",
    "dataTesting['sinopsis_processed'] = dataTesting['sinopsis'].apply(preprocess_text)\n",
    "\n",
    "X_test_submission_sequences = tokenizer.texts_to_sequences(dataTesting['sinopsis_processed'])\n",
    "\n",
    "X_test_submission_padded = pad_sequences(X_test_submission_sequences,maxlen=max_length, \n",
    "                               padding=padding_type, truncating=truncation_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_Action</th>\n",
       "      <th>p_Adventure</th>\n",
       "      <th>p_Animation</th>\n",
       "      <th>p_Biography</th>\n",
       "      <th>p_Comedy</th>\n",
       "      <th>p_Crime</th>\n",
       "      <th>p_Documentary</th>\n",
       "      <th>p_Drama</th>\n",
       "      <th>p_Family</th>\n",
       "      <th>p_Fantasy</th>\n",
       "      <th>...</th>\n",
       "      <th>p_Musical</th>\n",
       "      <th>p_Mystery</th>\n",
       "      <th>p_News</th>\n",
       "      <th>p_Romance</th>\n",
       "      <th>p_Sci-Fi</th>\n",
       "      <th>p_Short</th>\n",
       "      <th>p_Sport</th>\n",
       "      <th>p_Thriller</th>\n",
       "      <th>p_War</th>\n",
       "      <th>p_Western</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.134164</td>\n",
       "      <td>0.087074</td>\n",
       "      <td>0.024878</td>\n",
       "      <td>0.047566</td>\n",
       "      <td>0.388742</td>\n",
       "      <td>0.257270</td>\n",
       "      <td>0.042809</td>\n",
       "      <td>0.568447</td>\n",
       "      <td>0.063573</td>\n",
       "      <td>0.087191</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036734</td>\n",
       "      <td>0.106583</td>\n",
       "      <td>0.001113</td>\n",
       "      <td>0.297827</td>\n",
       "      <td>0.056587</td>\n",
       "      <td>0.010596</td>\n",
       "      <td>0.025261</td>\n",
       "      <td>0.204091</td>\n",
       "      <td>0.038663</td>\n",
       "      <td>0.025945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.260267</td>\n",
       "      <td>0.150722</td>\n",
       "      <td>0.026526</td>\n",
       "      <td>0.051781</td>\n",
       "      <td>0.286744</td>\n",
       "      <td>0.230790</td>\n",
       "      <td>0.063613</td>\n",
       "      <td>0.533125</td>\n",
       "      <td>0.071919</td>\n",
       "      <td>0.067926</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037149</td>\n",
       "      <td>0.114418</td>\n",
       "      <td>0.002079</td>\n",
       "      <td>0.190355</td>\n",
       "      <td>0.101284</td>\n",
       "      <td>0.014428</td>\n",
       "      <td>0.034403</td>\n",
       "      <td>0.375410</td>\n",
       "      <td>0.063920</td>\n",
       "      <td>0.039844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.287102</td>\n",
       "      <td>0.155477</td>\n",
       "      <td>0.029589</td>\n",
       "      <td>0.053116</td>\n",
       "      <td>0.287544</td>\n",
       "      <td>0.239995</td>\n",
       "      <td>0.067428</td>\n",
       "      <td>0.524082</td>\n",
       "      <td>0.073107</td>\n",
       "      <td>0.069455</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040298</td>\n",
       "      <td>0.128406</td>\n",
       "      <td>0.002459</td>\n",
       "      <td>0.188330</td>\n",
       "      <td>0.109945</td>\n",
       "      <td>0.016211</td>\n",
       "      <td>0.038031</td>\n",
       "      <td>0.409006</td>\n",
       "      <td>0.070264</td>\n",
       "      <td>0.045114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.251559</td>\n",
       "      <td>0.142144</td>\n",
       "      <td>0.027257</td>\n",
       "      <td>0.050830</td>\n",
       "      <td>0.286104</td>\n",
       "      <td>0.246902</td>\n",
       "      <td>0.063232</td>\n",
       "      <td>0.555720</td>\n",
       "      <td>0.067726</td>\n",
       "      <td>0.068071</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037316</td>\n",
       "      <td>0.117346</td>\n",
       "      <td>0.001891</td>\n",
       "      <td>0.195095</td>\n",
       "      <td>0.096194</td>\n",
       "      <td>0.014373</td>\n",
       "      <td>0.033095</td>\n",
       "      <td>0.381122</td>\n",
       "      <td>0.065452</td>\n",
       "      <td>0.038568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.204960</td>\n",
       "      <td>0.123655</td>\n",
       "      <td>0.027824</td>\n",
       "      <td>0.053675</td>\n",
       "      <td>0.321454</td>\n",
       "      <td>0.231040</td>\n",
       "      <td>0.052801</td>\n",
       "      <td>0.545847</td>\n",
       "      <td>0.070962</td>\n",
       "      <td>0.079493</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041755</td>\n",
       "      <td>0.129533</td>\n",
       "      <td>0.001747</td>\n",
       "      <td>0.232695</td>\n",
       "      <td>0.080112</td>\n",
       "      <td>0.013091</td>\n",
       "      <td>0.030983</td>\n",
       "      <td>0.308395</td>\n",
       "      <td>0.053565</td>\n",
       "      <td>0.036251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   p_Action  p_Adventure  p_Animation  p_Biography  p_Comedy   p_Crime  \\\n",
       "1  0.134164     0.087074     0.024878     0.047566  0.388742  0.257270   \n",
       "4  0.260267     0.150722     0.026526     0.051781  0.286744  0.230790   \n",
       "5  0.287102     0.155477     0.029589     0.053116  0.287544  0.239995   \n",
       "6  0.251559     0.142144     0.027257     0.050830  0.286104  0.246902   \n",
       "7  0.204960     0.123655     0.027824     0.053675  0.321454  0.231040   \n",
       "\n",
       "   p_Documentary   p_Drama  p_Family  p_Fantasy  ...  p_Musical  p_Mystery  \\\n",
       "1       0.042809  0.568447  0.063573   0.087191  ...   0.036734   0.106583   \n",
       "4       0.063613  0.533125  0.071919   0.067926  ...   0.037149   0.114418   \n",
       "5       0.067428  0.524082  0.073107   0.069455  ...   0.040298   0.128406   \n",
       "6       0.063232  0.555720  0.067726   0.068071  ...   0.037316   0.117346   \n",
       "7       0.052801  0.545847  0.070962   0.079493  ...   0.041755   0.129533   \n",
       "\n",
       "     p_News  p_Romance  p_Sci-Fi   p_Short   p_Sport  p_Thriller     p_War  \\\n",
       "1  0.001113   0.297827  0.056587  0.010596  0.025261    0.204091  0.038663   \n",
       "4  0.002079   0.190355  0.101284  0.014428  0.034403    0.375410  0.063920   \n",
       "5  0.002459   0.188330  0.109945  0.016211  0.038031    0.409006  0.070264   \n",
       "6  0.001891   0.195095  0.096194  0.014373  0.033095    0.381122  0.065452   \n",
       "7  0.001747   0.232695  0.080112  0.013091  0.030983    0.308395  0.053565   \n",
       "\n",
       "   p_Western  \n",
       "1   0.025945  \n",
       "4   0.039844  \n",
       "5   0.045114  \n",
       "6   0.038568  \n",
       "7   0.036251  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_test_dtm = X_test_dtm.toarray()\n",
    "# Predecir las etiquetas de género para el conjunto de prueba\n",
    "y_pred_test_genres = model.predict(X_test_submission_padded)\n",
    "\n",
    "cols = ['p_Action', 'p_Adventure', 'p_Animation', 'p_Biography', 'p_Comedy', 'p_Crime', 'p_Documentary', 'p_Drama', 'p_Family',\n",
    "        'p_Fantasy', 'p_Film-Noir', 'p_History', 'p_Horror', 'p_Music', 'p_Musical', 'p_Mystery', 'p_News', 'p_Romance',\n",
    "        'p_Sci-Fi', 'p_Short', 'p_Sport', 'p_Thriller', 'p_War', 'p_Western']\n",
    "res = pd.DataFrame(y_pred_test_genres, index=dataTesting.index, columns=cols)\n",
    "res.to_csv('pred_genres_text_RF.csv', index_label='ID')\n",
    "res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibracion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input\n",
    "from keras import metrics\n",
    "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import backend as K\n",
    "#from livelossplot import PlotLossesKeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de parámetros y sus valores sobre los que se va a calibrar\n",
    "nn_params = {\n",
    "    'output_activation': ['sigmoid', 'softmax'],\n",
    "    #'neurons_lstm':[64, 128, 256, 512],\n",
    "    #'neurons_dense':[64, 128, 256, 512],\n",
    "    'patience':[2, 5],\n",
    "    'loss':['binary_crossentropy', 'categorical_crossentropy']\n",
    "}\n",
    "\n",
    "# Definición de función que crea una red neuronal a partir de diferentes parámetros (nn_model_params)\n",
    "# En esta función se consideran 8 parámetos a calibrar, sin embargo se pueden agregar o quitar tantos como lo consideren pertinente\n",
    "def nn_model_params(output_activation,\n",
    "                    patience,\n",
    "                    loss):\n",
    "    \n",
    "    K.clear_session()\n",
    "    \n",
    "\n",
    "    # Definición red neuronal con la función Sequential()\n",
    "    # Definición de las capas de la red con el número de neuronas y la función de activación definidos en la función nn_model_params\n",
    "    model = Sequential([\n",
    "        embedding_layer,\n",
    "        Bidirectional(LSTM(150, return_sequences=True)), \n",
    "        Bidirectional(LSTM(150)),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(24, activation=output_activation)\n",
    "    ])\n",
    "\n",
    "    # Definición de función de perdida con parámetros definidos en la función nn_model_params\n",
    "    model.compile(optimizer = 'adamax', loss=loss, metrics=['accuracy'])\n",
    "    \n",
    "    # Definición de la función EarlyStopping con parámetro definido en la función nn_model_params\n",
    "    early_stopping = EarlyStopping(monitor=\"val_accuracy\", patience = patience)\n",
    "\n",
    "    # Entrenamiento de la red neuronal con parámetros definidos en la función nn_model_params\n",
    "    model.fit(X_train_padded, y_train_genres,\n",
    "              validation_data = (X_test_padded, y_test_genres),\n",
    "              epochs = 600,\n",
    "              batch_size=256,\n",
    "              callbacks=[early_stopping],\n",
    "              verbose=True\n",
    "             )\n",
    "\n",
    "     \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhoyoso\\.conda\\envs\\gpu_tenv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:305: UserWarning: The total space of parameters 8 is smaller than n_iter=10. Running 8 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "21/21 [==============================] - 11s 342ms/step - loss: 0.4162 - accuracy: 0.1267 - val_loss: 0.3016 - val_accuracy: 0.3120\n",
      "Epoch 2/600\n",
      "21/21 [==============================] - 6s 310ms/step - loss: 0.2939 - accuracy: 0.2276 - val_loss: 0.2958 - val_accuracy: 0.1926\n",
      "Epoch 3/600\n",
      "21/21 [==============================] - 6s 295ms/step - loss: 0.2911 - accuracy: 0.2002 - val_loss: 0.2931 - val_accuracy: 0.1926\n",
      "Epoch 1/600\n",
      "21/21 [==============================] - 12s 349ms/step - loss: 0.4475 - accuracy: 0.1558 - val_loss: 0.3029 - val_accuracy: 0.2214\n",
      "Epoch 2/600\n",
      "21/21 [==============================] - 7s 323ms/step - loss: 0.2944 - accuracy: 0.2040 - val_loss: 0.2949 - val_accuracy: 0.1926\n",
      "Epoch 3/600\n",
      "21/21 [==============================] - 7s 343ms/step - loss: 0.2904 - accuracy: 0.2002 - val_loss: 0.2911 - val_accuracy: 0.1926\n",
      "Epoch 1/600\n",
      "21/21 [==============================] - 12s 357ms/step - loss: 0.4511 - accuracy: 0.1588 - val_loss: 0.3065 - val_accuracy: 0.1926\n",
      "Epoch 2/600\n",
      "21/21 [==============================] - 7s 330ms/step - loss: 0.2952 - accuracy: 0.2002 - val_loss: 0.2959 - val_accuracy: 0.1926\n",
      "Epoch 3/600\n",
      "21/21 [==============================] - 7s 353ms/step - loss: 0.2919 - accuracy: 0.2002 - val_loss: 0.2951 - val_accuracy: 0.1926\n",
      "Epoch 4/600\n",
      "21/21 [==============================] - 8s 360ms/step - loss: 0.2914 - accuracy: 0.2002 - val_loss: 0.2943 - val_accuracy: 0.1926\n",
      "Epoch 5/600\n",
      "21/21 [==============================] - 7s 357ms/step - loss: 0.2897 - accuracy: 0.2004 - val_loss: 0.2916 - val_accuracy: 0.1922\n",
      "Epoch 6/600\n",
      "21/21 [==============================] - 7s 356ms/step - loss: 0.2857 - accuracy: 0.2008 - val_loss: 0.2873 - val_accuracy: 0.2111\n",
      "Epoch 7/600\n",
      "21/21 [==============================] - 7s 355ms/step - loss: 0.2810 - accuracy: 0.2484 - val_loss: 0.2824 - val_accuracy: 0.3108\n",
      "Epoch 8/600\n",
      "21/21 [==============================] - 7s 358ms/step - loss: 0.2745 - accuracy: 0.2808 - val_loss: 0.2779 - val_accuracy: 0.2222\n",
      "Epoch 9/600\n",
      "21/21 [==============================] - 7s 355ms/step - loss: 0.2675 - accuracy: 0.2760 - val_loss: 0.2701 - val_accuracy: 0.2506\n",
      "Epoch 10/600\n",
      "21/21 [==============================] - 7s 355ms/step - loss: 0.2596 - accuracy: 0.2898 - val_loss: 0.2635 - val_accuracy: 0.3078\n",
      "Epoch 11/600\n",
      "21/21 [==============================] - 7s 355ms/step - loss: 0.2530 - accuracy: 0.3144 - val_loss: 0.2562 - val_accuracy: 0.3189\n",
      "Epoch 12/600\n",
      "21/21 [==============================] - 8s 361ms/step - loss: 0.2474 - accuracy: 0.3231 - val_loss: 0.2540 - val_accuracy: 0.2928\n",
      "Epoch 13/600\n",
      "21/21 [==============================] - 8s 365ms/step - loss: 0.2404 - accuracy: 0.3532 - val_loss: 0.2498 - val_accuracy: 0.3642\n",
      "Epoch 14/600\n",
      "21/21 [==============================] - 8s 366ms/step - loss: 0.2342 - accuracy: 0.3558 - val_loss: 0.2414 - val_accuracy: 0.3546\n",
      "Epoch 15/600\n",
      "21/21 [==============================] - 8s 364ms/step - loss: 0.2292 - accuracy: 0.3613 - val_loss: 0.2436 - val_accuracy: 0.3066\n",
      "Epoch 16/600\n",
      "21/21 [==============================] - 8s 363ms/step - loss: 0.2272 - accuracy: 0.3422 - val_loss: 0.2364 - val_accuracy: 0.3285\n",
      "Epoch 17/600\n",
      "21/21 [==============================] - 8s 365ms/step - loss: 0.2203 - accuracy: 0.3657 - val_loss: 0.2327 - val_accuracy: 0.3542\n",
      "Epoch 18/600\n",
      "21/21 [==============================] - 8s 364ms/step - loss: 0.2151 - accuracy: 0.3787 - val_loss: 0.2292 - val_accuracy: 0.3753\n",
      "Epoch 19/600\n",
      "21/21 [==============================] - 8s 369ms/step - loss: 0.2117 - accuracy: 0.3804 - val_loss: 0.2282 - val_accuracy: 0.3691\n",
      "Epoch 20/600\n",
      "21/21 [==============================] - 8s 368ms/step - loss: 0.2094 - accuracy: 0.3942 - val_loss: 0.2301 - val_accuracy: 0.3668\n",
      "Epoch 21/600\n",
      "21/21 [==============================] - 8s 367ms/step - loss: 0.2057 - accuracy: 0.3899 - val_loss: 0.2257 - val_accuracy: 0.3768\n",
      "Epoch 22/600\n",
      "21/21 [==============================] - 8s 368ms/step - loss: 0.2037 - accuracy: 0.4020 - val_loss: 0.2261 - val_accuracy: 0.3787\n",
      "Epoch 23/600\n",
      "21/21 [==============================] - 8s 366ms/step - loss: 0.2005 - accuracy: 0.3995 - val_loss: 0.2282 - val_accuracy: 0.3254\n",
      "Epoch 24/600\n",
      "21/21 [==============================] - 8s 366ms/step - loss: 0.1964 - accuracy: 0.4131 - val_loss: 0.2231 - val_accuracy: 0.3757\n",
      "Epoch 25/600\n",
      "21/21 [==============================] - 8s 367ms/step - loss: 0.1942 - accuracy: 0.4203 - val_loss: 0.2204 - val_accuracy: 0.3849\n",
      "Epoch 26/600\n",
      "21/21 [==============================] - 8s 368ms/step - loss: 0.1908 - accuracy: 0.4267 - val_loss: 0.2223 - val_accuracy: 0.3730\n",
      "Epoch 27/600\n",
      "21/21 [==============================] - 8s 364ms/step - loss: 0.1881 - accuracy: 0.4286 - val_loss: 0.2197 - val_accuracy: 0.3672\n",
      "Epoch 28/600\n",
      "21/21 [==============================] - 8s 363ms/step - loss: 0.1869 - accuracy: 0.4352 - val_loss: 0.2218 - val_accuracy: 0.4279\n",
      "Epoch 29/600\n",
      "21/21 [==============================] - 8s 367ms/step - loss: 0.1843 - accuracy: 0.4381 - val_loss: 0.2186 - val_accuracy: 0.3972\n",
      "Epoch 30/600\n",
      "21/21 [==============================] - 8s 368ms/step - loss: 0.1801 - accuracy: 0.4494 - val_loss: 0.2173 - val_accuracy: 0.3987\n",
      "Epoch 31/600\n",
      "21/21 [==============================] - 8s 367ms/step - loss: 0.1770 - accuracy: 0.4513 - val_loss: 0.2198 - val_accuracy: 0.3672\n",
      "Epoch 32/600\n",
      "21/21 [==============================] - 8s 366ms/step - loss: 0.1768 - accuracy: 0.4536 - val_loss: 0.2205 - val_accuracy: 0.3599\n",
      "Epoch 33/600\n",
      "21/21 [==============================] - 8s 366ms/step - loss: 0.1746 - accuracy: 0.4591 - val_loss: 0.2173 - val_accuracy: 0.3964\n",
      "Epoch 1/600\n",
      "21/21 [==============================] - 12s 360ms/step - loss: 0.4851 - accuracy: 0.0314 - val_loss: 0.3177 - val_accuracy: 0.1926\n",
      "Epoch 2/600\n",
      "21/21 [==============================] - 7s 346ms/step - loss: 0.2991 - accuracy: 0.2002 - val_loss: 0.2966 - val_accuracy: 0.1926\n",
      "Epoch 3/600\n",
      "21/21 [==============================] - 7s 347ms/step - loss: 0.2923 - accuracy: 0.2002 - val_loss: 0.2947 - val_accuracy: 0.1926\n",
      "Epoch 4/600\n",
      "21/21 [==============================] - 7s 345ms/step - loss: 0.2908 - accuracy: 0.2002 - val_loss: 0.2931 - val_accuracy: 0.1926\n",
      "Epoch 5/600\n",
      "21/21 [==============================] - 7s 357ms/step - loss: 0.2876 - accuracy: 0.2004 - val_loss: 0.2891 - val_accuracy: 0.1926\n",
      "Epoch 6/600\n",
      "21/21 [==============================] - 7s 357ms/step - loss: 0.2817 - accuracy: 0.2008 - val_loss: 0.2822 - val_accuracy: 0.1965\n",
      "Epoch 7/600\n",
      "21/21 [==============================] - 7s 357ms/step - loss: 0.2735 - accuracy: 0.2097 - val_loss: 0.2733 - val_accuracy: 0.1922\n",
      "Epoch 8/600\n",
      "21/21 [==============================] - 7s 357ms/step - loss: 0.2641 - accuracy: 0.2114 - val_loss: 0.2676 - val_accuracy: 0.2053\n",
      "Epoch 9/600\n",
      "21/21 [==============================] - 7s 357ms/step - loss: 0.2565 - accuracy: 0.2333 - val_loss: 0.2616 - val_accuracy: 0.2076\n",
      "Epoch 10/600\n",
      "21/21 [==============================] - 8s 365ms/step - loss: 0.2508 - accuracy: 0.2653 - val_loss: 0.2557 - val_accuracy: 0.2736\n",
      "Epoch 11/600\n",
      "21/21 [==============================] - 8s 371ms/step - loss: 0.2454 - accuracy: 0.3055 - val_loss: 0.2518 - val_accuracy: 0.3212\n",
      "Epoch 12/600\n",
      "21/21 [==============================] - 8s 365ms/step - loss: 0.2392 - accuracy: 0.3192 - val_loss: 0.2480 - val_accuracy: 0.3012\n",
      "Epoch 13/600\n",
      "21/21 [==============================] - 8s 364ms/step - loss: 0.2353 - accuracy: 0.3243 - val_loss: 0.2436 - val_accuracy: 0.3104\n",
      "Epoch 14/600\n",
      "21/21 [==============================] - 8s 364ms/step - loss: 0.2301 - accuracy: 0.3314 - val_loss: 0.2403 - val_accuracy: 0.3415\n",
      "Epoch 15/600\n",
      "21/21 [==============================] - 8s 362ms/step - loss: 0.2273 - accuracy: 0.3502 - val_loss: 0.2394 - val_accuracy: 0.3477\n",
      "Epoch 16/600\n",
      "21/21 [==============================] - 8s 363ms/step - loss: 0.2246 - accuracy: 0.3706 - val_loss: 0.2369 - val_accuracy: 0.3615\n",
      "Epoch 17/600\n",
      "21/21 [==============================] - 8s 363ms/step - loss: 0.2208 - accuracy: 0.3649 - val_loss: 0.2348 - val_accuracy: 0.3258\n",
      "Epoch 18/600\n",
      "21/21 [==============================] - 8s 365ms/step - loss: 0.2159 - accuracy: 0.3666 - val_loss: 0.2318 - val_accuracy: 0.3404\n",
      "Epoch 19/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 8s 365ms/step - loss: 0.2132 - accuracy: 0.3798 - val_loss: 0.2302 - val_accuracy: 0.3561\n",
      "Epoch 20/600\n",
      "21/21 [==============================] - 8s 364ms/step - loss: 0.2085 - accuracy: 0.3961 - val_loss: 0.2284 - val_accuracy: 0.3319\n",
      "Epoch 21/600\n",
      "21/21 [==============================] - 8s 370ms/step - loss: 0.2053 - accuracy: 0.3925 - val_loss: 0.2278 - val_accuracy: 0.3580\n",
      "Epoch 1/600\n",
      "21/21 [==============================] - 12s 367ms/step - loss: 0.4476 - accuracy: 0.0297 - val_loss: 0.3091 - val_accuracy: 0.1926\n",
      "Epoch 2/600\n",
      "21/21 [==============================] - 7s 346ms/step - loss: 0.2970 - accuracy: 0.2002 - val_loss: 0.2963 - val_accuracy: 0.1926\n",
      "Epoch 3/600\n",
      "21/21 [==============================] - 7s 357ms/step - loss: 0.2922 - accuracy: 0.2002 - val_loss: 0.2951 - val_accuracy: 0.1926\n",
      "Epoch 1/600\n",
      "21/21 [==============================] - 12s 360ms/step - loss: 0.3931 - accuracy: 0.2053 - val_loss: 0.2993 - val_accuracy: 0.1926\n",
      "Epoch 2/600\n",
      "21/21 [==============================] - 7s 349ms/step - loss: 0.2934 - accuracy: 0.2002 - val_loss: 0.2944 - val_accuracy: 0.1926\n",
      "Epoch 3/600\n",
      "21/21 [==============================] - 7s 348ms/step - loss: 0.2900 - accuracy: 0.2002 - val_loss: 0.2925 - val_accuracy: 0.1926\n",
      "Epoch 1/600\n",
      "21/21 [==============================] - 12s 360ms/step - loss: 0.4063 - accuracy: 0.1809 - val_loss: 0.3033 - val_accuracy: 0.1926\n",
      "Epoch 2/600\n",
      "21/21 [==============================] - 7s 360ms/step - loss: 0.2948 - accuracy: 0.2002 - val_loss: 0.2960 - val_accuracy: 0.1926\n",
      "Epoch 3/600\n",
      "21/21 [==============================] - 7s 356ms/step - loss: 0.2920 - accuracy: 0.2002 - val_loss: 0.2951 - val_accuracy: 0.1926\n",
      "Epoch 4/600\n",
      "21/21 [==============================] - 7s 356ms/step - loss: 0.2914 - accuracy: 0.2002 - val_loss: 0.2945 - val_accuracy: 0.1926\n",
      "Epoch 5/600\n",
      "21/21 [==============================] - 7s 358ms/step - loss: 0.2904 - accuracy: 0.2002 - val_loss: 0.2927 - val_accuracy: 0.1926\n",
      "Epoch 6/600\n",
      "21/21 [==============================] - 7s 354ms/step - loss: 0.2874 - accuracy: 0.2002 - val_loss: 0.2892 - val_accuracy: 0.1926\n",
      "Epoch 1/600\n",
      "21/21 [==============================] - 13s 362ms/step - loss: 0.4329 - accuracy: 0.0832 - val_loss: 0.3072 - val_accuracy: 0.1926\n",
      "Epoch 2/600\n",
      "21/21 [==============================] - 7s 350ms/step - loss: 0.2955 - accuracy: 0.2002 - val_loss: 0.2962 - val_accuracy: 0.1926\n",
      "Epoch 3/600\n",
      "21/21 [==============================] - 7s 348ms/step - loss: 0.2917 - accuracy: 0.2002 - val_loss: 0.2941 - val_accuracy: 0.1926\n",
      "Epoch 4/600\n",
      "21/21 [==============================] - 7s 350ms/step - loss: 0.2887 - accuracy: 0.2002 - val_loss: 0.2898 - val_accuracy: 0.1926\n",
      "Epoch 5/600\n",
      "21/21 [==============================] - 8s 368ms/step - loss: 0.2827 - accuracy: 0.2002 - val_loss: 0.2821 - val_accuracy: 0.1926\n",
      "Epoch 6/600\n",
      "21/21 [==============================] - 8s 367ms/step - loss: 0.2757 - accuracy: 0.1989 - val_loss: 0.2786 - val_accuracy: 0.1930\n",
      "Epoch 7/600\n",
      "21/21 [==============================] - 8s 367ms/step - loss: 0.2703 - accuracy: 0.2167 - val_loss: 0.2711 - val_accuracy: 0.2076\n",
      "Epoch 8/600\n",
      "21/21 [==============================] - 8s 366ms/step - loss: 0.2624 - accuracy: 0.2441 - val_loss: 0.2630 - val_accuracy: 0.2433\n",
      "Epoch 9/600\n",
      "21/21 [==============================] - 8s 365ms/step - loss: 0.2548 - accuracy: 0.2725 - val_loss: 0.2588 - val_accuracy: 0.2640\n",
      "Epoch 10/600\n",
      "21/21 [==============================] - 8s 364ms/step - loss: 0.2507 - accuracy: 0.2834 - val_loss: 0.2552 - val_accuracy: 0.2329\n",
      "Epoch 11/600\n",
      "21/21 [==============================] - 8s 366ms/step - loss: 0.2448 - accuracy: 0.2883 - val_loss: 0.2489 - val_accuracy: 0.2782\n",
      "Epoch 12/600\n",
      "21/21 [==============================] - 8s 368ms/step - loss: 0.2374 - accuracy: 0.3059 - val_loss: 0.2444 - val_accuracy: 0.3158\n",
      "Epoch 13/600\n",
      "21/21 [==============================] - 8s 365ms/step - loss: 0.2325 - accuracy: 0.3173 - val_loss: 0.2432 - val_accuracy: 0.3093\n",
      "Epoch 14/600\n",
      "21/21 [==============================] - 8s 361ms/step - loss: 0.2284 - accuracy: 0.3161 - val_loss: 0.2375 - val_accuracy: 0.3223\n",
      "Epoch 15/600\n",
      "21/21 [==============================] - 8s 368ms/step - loss: 0.2235 - accuracy: 0.3314 - val_loss: 0.2354 - val_accuracy: 0.3104\n",
      "Epoch 16/600\n",
      "21/21 [==============================] - 8s 362ms/step - loss: 0.2185 - accuracy: 0.3443 - val_loss: 0.2304 - val_accuracy: 0.3335\n",
      "Epoch 17/600\n",
      "21/21 [==============================] - 8s 360ms/step - loss: 0.2133 - accuracy: 0.3685 - val_loss: 0.2280 - val_accuracy: 0.3711\n",
      "Epoch 18/600\n",
      "21/21 [==============================] - 8s 365ms/step - loss: 0.2096 - accuracy: 0.3817 - val_loss: 0.2245 - val_accuracy: 0.3346\n",
      "Epoch 19/600\n",
      "21/21 [==============================] - 8s 366ms/step - loss: 0.2056 - accuracy: 0.3893 - val_loss: 0.2230 - val_accuracy: 0.4021\n",
      "Epoch 20/600\n",
      "21/21 [==============================] - 8s 364ms/step - loss: 0.2017 - accuracy: 0.3995 - val_loss: 0.2212 - val_accuracy: 0.3738\n",
      "Epoch 21/600\n",
      "21/21 [==============================] - 8s 367ms/step - loss: 0.1979 - accuracy: 0.4067 - val_loss: 0.2211 - val_accuracy: 0.3795\n",
      "Epoch 22/600\n",
      "21/21 [==============================] - 8s 367ms/step - loss: 0.1965 - accuracy: 0.4122 - val_loss: 0.2191 - val_accuracy: 0.3546\n",
      "Epoch 23/600\n",
      "21/21 [==============================] - 8s 365ms/step - loss: 0.1941 - accuracy: 0.4203 - val_loss: 0.2179 - val_accuracy: 0.3626\n",
      "Epoch 24/600\n",
      "21/21 [==============================] - 8s 362ms/step - loss: 0.1903 - accuracy: 0.4146 - val_loss: 0.2163 - val_accuracy: 0.3964\n",
      "Epoch 1/600\n",
      "21/21 [==============================] - 12s 363ms/step - loss: 9.8918 - accuracy: 0.1949 - val_loss: 14.2714 - val_accuracy: 0.3120\n",
      "Epoch 2/600\n",
      "21/21 [==============================] - 7s 336ms/step - loss: 16.7205 - accuracy: 0.2006 - val_loss: 20.5806 - val_accuracy: 0.1926\n",
      "Epoch 3/600\n",
      "21/21 [==============================] - 8s 369ms/step - loss: 23.0843 - accuracy: 0.2154 - val_loss: 27.7843 - val_accuracy: 0.1926\n",
      "Epoch 1/600\n",
      "21/21 [==============================] - 12s 359ms/step - loss: 9.4555 - accuracy: 0.1015 - val_loss: 12.9570 - val_accuracy: 0.1926\n",
      "Epoch 2/600\n",
      "21/21 [==============================] - 7s 339ms/step - loss: 16.6105 - accuracy: 0.2394 - val_loss: 21.3408 - val_accuracy: 0.1926\n",
      "Epoch 3/600\n",
      "21/21 [==============================] - 7s 340ms/step - loss: 24.1069 - accuracy: 0.2002 - val_loss: 28.9590 - val_accuracy: 0.1926\n",
      "Epoch 1/600\n",
      "21/21 [==============================] - 12s 362ms/step - loss: 9.3100 - accuracy: 0.1872 - val_loss: 12.3877 - val_accuracy: 0.3120\n",
      "Epoch 2/600\n",
      "21/21 [==============================] - 7s 331ms/step - loss: 13.5689 - accuracy: 0.2252 - val_loss: 15.7720 - val_accuracy: 0.3120\n",
      "Epoch 3/600\n",
      "21/21 [==============================] - 8s 362ms/step - loss: 16.2374 - accuracy: 0.2293 - val_loss: 18.1934 - val_accuracy: 0.1926\n",
      "Epoch 4/600\n",
      "21/21 [==============================] - 7s 344ms/step - loss: 18.8544 - accuracy: 0.2515 - val_loss: 21.6477 - val_accuracy: 0.1926\n",
      "Epoch 5/600\n",
      "21/21 [==============================] - 7s 344ms/step - loss: 22.4719 - accuracy: 0.2091 - val_loss: 25.8569 - val_accuracy: 0.1926\n",
      "Epoch 6/600\n",
      "21/21 [==============================] - 8s 363ms/step - loss: 26.1082 - accuracy: 0.2002 - val_loss: 29.8397 - val_accuracy: 0.1926\n",
      "Epoch 1/600\n",
      "21/21 [==============================] - 12s 360ms/step - loss: 8.9253 - accuracy: 0.2698 - val_loss: 11.9540 - val_accuracy: 0.1926\n",
      "Epoch 2/600\n",
      "21/21 [==============================] - 7s 332ms/step - loss: 14.6287 - accuracy: 0.2696 - val_loss: 18.1951 - val_accuracy: 0.1926\n",
      "Epoch 3/600\n",
      "21/21 [==============================] - 7s 358ms/step - loss: 19.9318 - accuracy: 0.2477 - val_loss: 23.2752 - val_accuracy: 0.1926\n",
      "Epoch 4/600\n",
      "21/21 [==============================] - 7s 342ms/step - loss: 25.5885 - accuracy: 0.2477 - val_loss: 30.0345 - val_accuracy: 0.1926\n",
      "Epoch 5/600\n",
      "21/21 [==============================] - 7s 354ms/step - loss: 32.6239 - accuracy: 0.2002 - val_loss: 37.8127 - val_accuracy: 0.1926\n",
      "Epoch 6/600\n",
      "21/21 [==============================] - 8s 365ms/step - loss: 39.9628 - accuracy: 0.2002 - val_loss: 45.0656 - val_accuracy: 0.1926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "21/21 [==============================] - 12s 365ms/step - loss: 9.7305 - accuracy: 0.2118 - val_loss: 14.6878 - val_accuracy: 0.3120\n",
      "Epoch 2/600\n",
      "21/21 [==============================] - 7s 337ms/step - loss: 15.9351 - accuracy: 0.2276 - val_loss: 19.5675 - val_accuracy: 0.1926\n",
      "Epoch 3/600\n",
      "21/21 [==============================] - 8s 365ms/step - loss: 21.5420 - accuracy: 0.2002 - val_loss: 26.6291 - val_accuracy: 0.1926\n",
      "Epoch 1/600\n",
      "21/21 [==============================] - 12s 358ms/step - loss: 9.7332 - accuracy: 0.1938 - val_loss: 14.0449 - val_accuracy: 0.3120\n",
      "Epoch 2/600\n",
      "21/21 [==============================] - 7s 331ms/step - loss: 16.0279 - accuracy: 0.2169 - val_loss: 18.6681 - val_accuracy: 0.1926\n",
      "Epoch 3/600\n",
      "21/21 [==============================] - 7s 350ms/step - loss: 21.0935 - accuracy: 0.2002 - val_loss: 24.5766 - val_accuracy: 0.1926\n",
      "Epoch 1/600\n",
      "21/21 [==============================] - 12s 362ms/step - loss: 9.7778 - accuracy: 0.1997 - val_loss: 13.6433 - val_accuracy: 0.3120\n",
      "Epoch 2/600\n",
      "21/21 [==============================] - 7s 332ms/step - loss: 16.6532 - accuracy: 0.2152 - val_loss: 20.8345 - val_accuracy: 0.1992\n",
      "Epoch 3/600\n",
      "21/21 [==============================] - 7s 349ms/step - loss: 23.6537 - accuracy: 0.2002 - val_loss: 28.6043 - val_accuracy: 0.1926\n",
      "Epoch 4/600\n",
      "21/21 [==============================] - 7s 348ms/step - loss: 30.3647 - accuracy: 0.2356 - val_loss: 34.4780 - val_accuracy: 0.1926\n",
      "Epoch 5/600\n",
      "21/21 [==============================] - 7s 343ms/step - loss: 35.0199 - accuracy: 0.2157 - val_loss: 38.7645 - val_accuracy: 0.1926\n",
      "Epoch 6/600\n",
      "21/21 [==============================] - 7s 353ms/step - loss: 38.5898 - accuracy: 0.2002 - val_loss: 42.4179 - val_accuracy: 0.1926\n",
      "Epoch 1/600\n",
      "21/21 [==============================] - 12s 361ms/step - loss: 8.9795 - accuracy: 0.2500 - val_loss: 12.0405 - val_accuracy: 0.1926\n",
      "Epoch 2/600\n",
      "21/21 [==============================] - 7s 332ms/step - loss: 14.6841 - accuracy: 0.2290 - val_loss: 17.6205 - val_accuracy: 0.1926\n",
      "Epoch 3/600\n",
      "21/21 [==============================] - 7s 345ms/step - loss: 19.6096 - accuracy: 0.2356 - val_loss: 23.3746 - val_accuracy: 0.1926\n",
      "Epoch 4/600\n",
      "21/21 [==============================] - 8s 363ms/step - loss: 25.3392 - accuracy: 0.2278 - val_loss: 28.9675 - val_accuracy: 0.1926\n",
      "Epoch 5/600\n",
      "21/21 [==============================] - 7s 357ms/step - loss: 30.7577 - accuracy: 0.2002 - val_loss: 35.0818 - val_accuracy: 0.1926\n",
      "Epoch 6/600\n",
      "21/21 [==============================] - 7s 346ms/step - loss: 37.2405 - accuracy: 0.2002 - val_loss: 42.1179 - val_accuracy: 0.1926\n",
      "Epoch 1/600\n",
      "21/21 [==============================] - 12s 364ms/step - loss: 0.4350 - accuracy: 0.0741 - val_loss: 0.3075 - val_accuracy: 0.1926\n",
      "Epoch 2/600\n",
      "21/21 [==============================] - 7s 345ms/step - loss: 0.2961 - accuracy: 0.2004 - val_loss: 0.2958 - val_accuracy: 0.1926\n",
      "Epoch 3/600\n",
      "21/21 [==============================] - 8s 361ms/step - loss: 0.2921 - accuracy: 0.2002 - val_loss: 0.2950 - val_accuracy: 0.1926\n",
      "Epoch 4/600\n",
      "21/21 [==============================] - 7s 358ms/step - loss: 0.2913 - accuracy: 0.2002 - val_loss: 0.2938 - val_accuracy: 0.1926\n",
      "Epoch 5/600\n",
      "21/21 [==============================] - 7s 358ms/step - loss: 0.2890 - accuracy: 0.2002 - val_loss: 0.2907 - val_accuracy: 0.1926\n",
      "Epoch 6/600\n",
      "21/21 [==============================] - 7s 359ms/step - loss: 0.2836 - accuracy: 0.2002 - val_loss: 0.2837 - val_accuracy: 0.1922\n",
      "Los mejores parametros segun Randomnized Search: {'patience': 5, 'output_activation': 'sigmoid', 'loss': 'binary_crossentropy'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Definición de red neuronal usando el wrapper KerasRegressor y usando como argumento build_fn en la función nn_model_params\n",
    "nn_model = KerasClassifier(build_fn=nn_model_params, verbose=0)\n",
    "\n",
    "# Definición método GridSearch para la calibración de parámetros definidos en nn_params\n",
    "rs = RandomizedSearchCV(nn_model, param_distributions=nn_params, n_iter=10, cv = 2)\n",
    "rs.fit(X_train_padded, y_train_genres)\n",
    "\n",
    "print('Los mejores parametros segun Randomnized Search:', rs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "21/21 [==============================] - 12s 340ms/step - loss: 0.4257 - accuracy: 0.1089 - val_loss: 0.3026 - val_accuracy: 0.1926\n",
      "Epoch 2/600\n",
      "21/21 [==============================] - 6s 295ms/step - loss: 0.2941 - accuracy: 0.2002 - val_loss: 0.2951 - val_accuracy: 0.1926\n",
      "Epoch 3/600\n",
      "21/21 [==============================] - 6s 298ms/step - loss: 0.2906 - accuracy: 0.2108 - val_loss: 0.2921 - val_accuracy: 0.1930\n",
      "Epoch 4/600\n",
      "21/21 [==============================] - 6s 305ms/step - loss: 0.2858 - accuracy: 0.2486 - val_loss: 0.2849 - val_accuracy: 0.3135\n",
      "Epoch 5/600\n",
      "21/21 [==============================] - 7s 320ms/step - loss: 0.2761 - accuracy: 0.3038 - val_loss: 0.2743 - val_accuracy: 0.3365\n",
      "Epoch 6/600\n",
      "21/21 [==============================] - 7s 320ms/step - loss: 0.2654 - accuracy: 0.3466 - val_loss: 0.2704 - val_accuracy: 0.2556\n",
      "Epoch 7/600\n",
      "21/21 [==============================] - 7s 327ms/step - loss: 0.2579 - accuracy: 0.3284 - val_loss: 0.2581 - val_accuracy: 0.3281\n",
      "Epoch 8/600\n",
      "21/21 [==============================] - 7s 322ms/step - loss: 0.2464 - accuracy: 0.3407 - val_loss: 0.2481 - val_accuracy: 0.3254\n",
      "Epoch 9/600\n",
      "21/21 [==============================] - 7s 315ms/step - loss: 0.2382 - accuracy: 0.3314 - val_loss: 0.2448 - val_accuracy: 0.3446\n",
      "Epoch 10/600\n",
      "21/21 [==============================] - 7s 337ms/step - loss: 0.2320 - accuracy: 0.3382 - val_loss: 0.2385 - val_accuracy: 0.3150\n",
      "Epoch 11/600\n",
      "21/21 [==============================] - 7s 327ms/step - loss: 0.2248 - accuracy: 0.3486 - val_loss: 0.2342 - val_accuracy: 0.3350\n",
      "Epoch 12/600\n",
      "21/21 [==============================] - 7s 345ms/step - loss: 0.2188 - accuracy: 0.3676 - val_loss: 0.2310 - val_accuracy: 0.3703\n",
      "Epoch 13/600\n",
      "21/21 [==============================] - 7s 338ms/step - loss: 0.2130 - accuracy: 0.3770 - val_loss: 0.2302 - val_accuracy: 0.3208\n",
      "Epoch 14/600\n",
      "21/21 [==============================] - 7s 334ms/step - loss: 0.2092 - accuracy: 0.3904 - val_loss: 0.2258 - val_accuracy: 0.3918\n",
      "Epoch 15/600\n",
      "21/21 [==============================] - 7s 339ms/step - loss: 0.2049 - accuracy: 0.4046 - val_loss: 0.2245 - val_accuracy: 0.3707\n",
      "Epoch 16/600\n",
      "21/21 [==============================] - 7s 322ms/step - loss: 0.2007 - accuracy: 0.4173 - val_loss: 0.2214 - val_accuracy: 0.3799\n",
      "Epoch 17/600\n",
      "21/21 [==============================] - 7s 348ms/step - loss: 0.1988 - accuracy: 0.4201 - val_loss: 0.2255 - val_accuracy: 0.3473\n",
      "Epoch 18/600\n",
      "21/21 [==============================] - 7s 334ms/step - loss: 0.1944 - accuracy: 0.4318 - val_loss: 0.2210 - val_accuracy: 0.3964\n",
      "Epoch 19/600\n",
      "21/21 [==============================] - 7s 341ms/step - loss: 0.1925 - accuracy: 0.4351 - val_loss: 0.2206 - val_accuracy: 0.3937\n",
      "Epoch 20/600\n",
      "21/21 [==============================] - 7s 343ms/step - loss: 0.1888 - accuracy: 0.4388 - val_loss: 0.2192 - val_accuracy: 0.3998\n",
      "Epoch 21/600\n",
      "21/21 [==============================] - 7s 338ms/step - loss: 0.1845 - accuracy: 0.4492 - val_loss: 0.2186 - val_accuracy: 0.3964\n",
      "Epoch 22/600\n",
      "21/21 [==============================] - 7s 341ms/step - loss: 0.1809 - accuracy: 0.4490 - val_loss: 0.2179 - val_accuracy: 0.4068\n",
      "Epoch 23/600\n",
      "21/21 [==============================] - 7s 353ms/step - loss: 0.1787 - accuracy: 0.4587 - val_loss: 0.2210 - val_accuracy: 0.3665\n",
      "Epoch 24/600\n",
      "21/21 [==============================] - 7s 349ms/step - loss: 0.1768 - accuracy: 0.4611 - val_loss: 0.2188 - val_accuracy: 0.4045\n",
      "Epoch 25/600\n",
      "21/21 [==============================] - 7s 336ms/step - loss: 0.1728 - accuracy: 0.4589 - val_loss: 0.2182 - val_accuracy: 0.4056\n",
      "Epoch 26/600\n",
      "21/21 [==============================] - 8s 362ms/step - loss: 0.1698 - accuracy: 0.4638 - val_loss: 0.2200 - val_accuracy: 0.3780\n",
      "Epoch 27/600\n",
      "21/21 [==============================] - 7s 349ms/step - loss: 0.1685 - accuracy: 0.4700 - val_loss: 0.2188 - val_accuracy: 0.3918\n"
     ]
    }
   ],
   "source": [
    "# Desempeño de modelo con los parametros de Randomized Search\n",
    "# {'patience': 5, 'output_activation': 'sigmoid', 'loss': 'binary_crossentropy'}\n",
    "model = nn_model_params(\n",
    "                        patience=5,\n",
    "                        output_activation='sigmoid',\n",
    "                        loss='binary_crossentropy'\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82/82 [==============================] - 3s 36ms/step - loss: 0.2188 - accuracy: 0.3918\n",
      "Test accuracy : 0.39178818464279175\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test_padded,y_test_genres)\n",
    "print('Test accuracy :', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el Modelo\n",
    "model.save('final_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Guardar el tokenizador\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "# Recrea exactamente el mismo modelo solo desde el archivo\n",
    "new_model = keras.models.load_model('final_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    new_tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_Action</th>\n",
       "      <th>p_Adventure</th>\n",
       "      <th>p_Animation</th>\n",
       "      <th>p_Biography</th>\n",
       "      <th>p_Comedy</th>\n",
       "      <th>p_Crime</th>\n",
       "      <th>p_Documentary</th>\n",
       "      <th>p_Drama</th>\n",
       "      <th>p_Family</th>\n",
       "      <th>p_Fantasy</th>\n",
       "      <th>...</th>\n",
       "      <th>p_Musical</th>\n",
       "      <th>p_Mystery</th>\n",
       "      <th>p_News</th>\n",
       "      <th>p_Romance</th>\n",
       "      <th>p_Sci-Fi</th>\n",
       "      <th>p_Short</th>\n",
       "      <th>p_Sport</th>\n",
       "      <th>p_Thriller</th>\n",
       "      <th>p_War</th>\n",
       "      <th>p_Western</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.109952</td>\n",
       "      <td>0.024771</td>\n",
       "      <td>0.036794</td>\n",
       "      <td>0.004108</td>\n",
       "      <td>0.290189</td>\n",
       "      <td>0.042824</td>\n",
       "      <td>0.203385</td>\n",
       "      <td>0.027371</td>\n",
       "      <td>0.01051</td>\n",
       "      <td>0.072733</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006351</td>\n",
       "      <td>0.021919</td>\n",
       "      <td>0.000673</td>\n",
       "      <td>0.013873</td>\n",
       "      <td>0.085907</td>\n",
       "      <td>0.031826</td>\n",
       "      <td>0.001152</td>\n",
       "      <td>0.126808</td>\n",
       "      <td>0.002138</td>\n",
       "      <td>0.000535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   p_Action  p_Adventure  p_Animation  p_Biography  p_Comedy   p_Crime  \\\n",
       "0  0.109952     0.024771     0.036794     0.004108  0.290189  0.042824   \n",
       "\n",
       "   p_Documentary   p_Drama  p_Family  p_Fantasy  ...  p_Musical  p_Mystery  \\\n",
       "0       0.203385  0.027371   0.01051   0.072733  ...   0.006351   0.021919   \n",
       "\n",
       "     p_News  p_Romance  p_Sci-Fi   p_Short   p_Sport  p_Thriller     p_War  \\\n",
       "0  0.000673   0.013873  0.085907  0.031826  0.001152    0.126808  0.002138   \n",
       "\n",
       "   p_Western  \n",
       "0   0.000535  \n",
       "\n",
       "[1 rows x 24 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = ['horror movie']\n",
    "\n",
    "X_test_seq = new_tokenizer.texts_to_sequences(test)\n",
    "\n",
    "X_test_pad = pad_sequences(X_test_seq,maxlen=max_length, \n",
    "                               padding=padding_type, truncating=truncation_type)\n",
    "\n",
    "preds = new_model.predict(X_test_pad)\n",
    "\n",
    "cols = ['p_Action', 'p_Adventure', 'p_Animation', 'p_Biography', 'p_Comedy', 'p_Crime', 'p_Documentary', 'p_Drama', 'p_Family',\n",
    "        'p_Fantasy', 'p_Film-Noir', 'p_History', 'p_Horror', 'p_Music', 'p_Musical', 'p_Mystery', 'p_News', 'p_Romance',\n",
    "        'p_Sci-Fi', 'p_Short', 'p_Sport', 'p_Thriller', 'p_War', 'p_Western']\n",
    "\n",
    "\n",
    "prediccion = pd.DataFrame(preds, columns=cols)\n",
    "prediccion.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'p_Horror'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols[np.argmax(preds[0])]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gpu_tensorflow_env)",
   "language": "python",
   "name": "gpu_tenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
